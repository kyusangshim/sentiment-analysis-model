{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3dd535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13eea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Vader 분석기 초기화\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "\n",
    "def data_processing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https\\S+|www|\\S+https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@w+|\\#', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in text_tokens if not w in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stemming(data):\n",
    "    text = [stemmer.stem(word) for word in data]\n",
    "    return data\n",
    "\n",
    "\n",
    "train.text = train['text'].apply(data_processing)\n",
    "train['text'] = train['text'].apply(lambda x: stemming(x))\n",
    "test.text = test['text'].apply(data_processing)\n",
    "test['text'] = test['text'].apply(lambda x: stemming(x))\n",
    "\n",
    "\n",
    "Y_train = train['sentiment']\n",
    "\n",
    "X_test = test['text']\n",
    "\n",
    "# 각 행의 sentiment 값에 따라 처리\n",
    "filtered_sentences = []\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    text = row['text']\n",
    "    sentiment = row['sentiment']\n",
    "    \n",
    "    if sentiment != 2:  # sentiment 값이 2가 아닌 경우\n",
    "        filtered_sentences.append(text)\n",
    "    else:  # sentiment 값이 2인 경우 (부정인 경우)\n",
    "        words = text.split()\n",
    "        negative_words = [word for word in words if analyzer.polarity_scores(word)['compound'] < 0]\n",
    "        if negative_words:\n",
    "            filtered_sentence = ' '.join(negative_words)\n",
    "            filtered_sentences.append(filtered_sentence)\n",
    "        else:\n",
    "            filtered_sentences.append(\"\")\n",
    "\n",
    "# 결과 출력 (판다스 시리즈로 변환)\n",
    "X_train = pd.Series(filtered_sentences)\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https\\S+|www|\\S+https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@w+|\\#', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in text_tokens if not w in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stemming(data):\n",
    "    text = [stemmer.stem(word) for word in data]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "\n",
    "\n",
    "train.text = train['text'].apply(data_processing)\n",
    "train['text'] = train['text'].apply(lambda x: stemming(x))\n",
    "test.text = test['text'].apply(data_processing)\n",
    "test['text'] = test['text'].apply(lambda x: stemming(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train = train['text']\n",
    "Y_train = train['sentiment']\n",
    "\n",
    "X_test = test['text']\n",
    "\n",
    "\n",
    "\n",
    "## X_train 토큰화\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "text_series = X_train\n",
    "token_list = []\n",
    "for text in text_series:\n",
    "    tokens = tokenize_text(text)\n",
    "    token_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7858ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Vader 분석기 초기화\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# 각 행의 sentiment 값에 따라 처리\n",
    "filtered_sentences = []\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    text = row['text']\n",
    "    sentiment = row['sentiment']\n",
    "    \n",
    "    if sentiment != 2:  # sentiment 값이 2가 아닌 경우\n",
    "        filtered_sentences.append(text)\n",
    "    else:  # sentiment 값이 2인 경우 (부정인 경우)\n",
    "        words = text.split()\n",
    "        negative_words = [word for word in words if analyzer.polarity_scores(word)['compound'] < 0]\n",
    "        if negative_words:\n",
    "            filtered_sentence = ' '.join(negative_words)\n",
    "            filtered_sentences.append(filtered_sentence)\n",
    "        else:\n",
    "            filtered_sentences.append(\"\")\n",
    "\n",
    "# 결과 출력 (판다스 시리즈로 변환)\n",
    "result_series = pd.Series(filtered_sentences)\n",
    "print(result_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['id'],axis=1)\n",
    "train.groupby('sentiment').count().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pd.Series(token_list)\n",
    "\n",
    "frequencies = pd.Series(np.concatenate([w for w in token_list])).value_counts()\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_freq_count(tokens):\n",
    "    neg_token = tokens[train['sentiment'] == 2]\n",
    "    pos_token = tokens[train['sentiment'] == 1]\n",
    "    neul_token = tokens[train['sentiment'] == 0]\n",
    "    neg_freq = pd.Series(np.concatenate([w for w in neg_token])).value_counts()\n",
    "    pos_freq = pd.Series(np.concatenate([w for w in pos_token])).value_counts()\n",
    "    neul_freq = pd.Series(np.concatenate([w for w in neul_token])).value_counts()\n",
    "    return neg_freq, pos_freq, neul_freq\n",
    "\n",
    "def remove_doubled_words(neg_freq ,pos_freq, neul_freq, tokens):\n",
    "    top_50_neg = neg_freq[:20]\n",
    "    top_50_pos = pos_freq[:20]\n",
    "    top_50_neul = neul_freq[:20]\n",
    "    remove_word = [p for p in top_50_pos.index if p in top_50_neg.index]\n",
    "    tokens_removed = remove_stop_words(tokens, remove_word)\n",
    "    final_tokens = cleaning_tokens(tokens_removed)\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "def draw_top_hist(data,name,color): \n",
    "    top_10 = data[:10]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.suptitle(\"Bar Plot\", fontsize=30)\n",
    "    plt.rc('font', family='Arial')\n",
    "    plt.title(name, fontsize=15)\n",
    "    plt.bar(top_10.index, top_10.values, width=0.5, color=color, alpha=0.5)\n",
    "    plt.xticks(top_10.index, fontsize=15)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_freq ,pos_freq, neul_freq = get_freq_count(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6beb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_50_pos = pos_freq[:3]\n",
    "top_50_neg = neg_freq[:3]\n",
    "top_50_neul = neul_freq[:3]\n",
    "common_words = [p for p in top_50_neg.index if p in set.intersection(set(top_50_pos.index), set(top_50_neul.index))]\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484de209",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_token_list = []\n",
    "common_neg_words = ['like', 'get', 'go']\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    token = tokens.iloc[i]  # 각 토큰 가져오기\n",
    "    sentiment = train['sentiment'].iloc[i]  # 해당 토큰의 sentiment 값 가져오기\n",
    "    \n",
    "    # 해당 토큰의 sentiment가 2인 경우 common_neg_words를 사용하여 필터링\n",
    "    if sentiment == 2:\n",
    "        clean_token_neg = list(filter(lambda x: x not in common_neg_words, token))\n",
    "        clean_token = list(filter(lambda x: x not in common_words, clean_token_neg))\n",
    "    else:\n",
    "        clean_token = list(filter(lambda x: x not in common_words, token))\n",
    "    \n",
    "    clean_token_list.append(clean_token)\n",
    "\n",
    "clean_tokens = pd.Series(clean_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3704e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagged_tokens = clean_tokens.apply(lambda tokens: pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e963c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CD(숫자를 나타내는 품사), NNP(고유명사. 단수형), NNPS(고유명사, 복수형) tag 제거 후 Series객체로 반환\n",
    "\n",
    "filtered_series = [[word for word, tag in tagged_tokens if tag not in ['CD','NNP','NNPS']] for tagged_tokens in tagged_tokens]\n",
    "filtered_series = pd.Series(filtered_series)\n",
    "\n",
    "\n",
    "## 토큰화 되어있는 Series객체 토큰들 합치기\n",
    "\n",
    "X_train = filtered_series.apply(lambda tokens:' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_train_vec, Y_train, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d814dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 포레스트 분류기\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 그래디언트 부스팅 분류기\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 포레스트 분류기\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 그래디언트 부스팅 분류기\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "ensemble_classifier = VotingClassifier(estimators=[\n",
    "    ('rf', rf_classifier),\n",
    "    ('gb', gb_classifier)\n",
    "], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee842b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 확률 계산 함수\n",
    "def get_emotion_probabilities(text, model, vectorizer):\n",
    "    # 텍스트 데이터를 TF-IDF 벡터로 변환\n",
    "    text_tfidf = vectorizer.transform([text])\n",
    "    \n",
    "    # 각 클래스(감정)에 대한 확률 예측\n",
    "    probabilities = model.predict_proba(text_tfidf)\n",
    "    \n",
    "    # 클래스에 따른 확률을 딕셔너리로 반환\n",
    "    emotion_probabilities = {\n",
    "        \"중립\": probabilities[0][0],\n",
    "        \"긍정\": probabilities[0][1],\n",
    "        \"부정\": probabilities[0][2]\n",
    "    }\n",
    "    \n",
    "    return emotion_probabilities\n",
    "\n",
    "# 텍스트 감정 분류 예측 예제\n",
    "new_text = \"like\"\n",
    "predicted_emotion_probabilities = get_emotion_probabilities(new_text, ensemble_classifier, vectorizer)\n",
    "\n",
    "# 예측된 각각의 확률 출력\n",
    "for emotion, probability in predicted_emotion_probabilities.items():\n",
    "    print(f\"{emotion} 확률: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c19b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_pred = ensemble_classifier.predict(x_test)\n",
    "print(classification_report(y_test, Y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f1f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = ensemble_classifier.predict(X_test_vec)\n",
    "\n",
    "# 결과 저장\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['sentiment'] = Y_test_pred\n",
    "submit.to_csv('./ensemble_sentiment17.csv', index=False)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d54fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a7aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a7124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
